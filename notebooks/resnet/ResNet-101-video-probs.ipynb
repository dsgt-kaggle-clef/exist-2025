{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6edd9e-902f-4cb2-8b29-8fda485e4309",
   "metadata": {},
   "source": [
    "# ResNet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea1e14f-6151-4a72-ab2b-c707244742ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd4f8bb-e943-4706-87c9-1189e9444f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eaf6536-3832-4546-96e0-6f9a4c87ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the path for testing on my local machine with a single file\n",
    "#base_path = '../../../../dat/EXIST_2025_Videos_Dataset/training/'\n",
    "\n",
    "# This is the path for testing on my local machine with train, val, test files\n",
    "base_path = '../../../../dat/EXIST_2025_Videos_Dataset/train_test/'\n",
    "base_vid_path = '../../../../dat/EXIST_2025_Videos_Dataset/training/'\n",
    "\n",
    "# This is the path for running on the PACE Cluster\n",
    "#base_path = '/storage/coda1/p-dsgt_clef2025/0/shared/exist/latest/EXIST 2025 Memes Dataset/training/'\n",
    "\n",
    "file_name = 'EXIST2025_training.json'\n",
    "train_file_name = 'train_df.csv'\n",
    "val_file_name = 'valid_df.csv'\n",
    "test_file_name = 'test_df.csv'\n",
    "\n",
    "#print(\"path:\", train_path)\n",
    "video_path_col = 'path_video'\n",
    "#target_col = 'labels_task3_1'\n",
    "target_col = 'target'\n",
    "#id_col = 'id_EXIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53a0d49-5bff-4f6e-b5a3-aa6ef5181433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, df, transform, max_frames=16):\n",
    "        #self.video_paths = video_paths\n",
    "        #self.labels = labels\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        print(len(self.df))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_video_frames(self, video_path):\n",
    "        #print(\"Loading:\", video_path)\n",
    "        import cv2\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        success, frame = cap.read()\n",
    "        while success and len(frames) < self.max_frames:\n",
    "            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frames.append(self.transform(img))\n",
    "            success, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        # Pad with zeros if too short\n",
    "        while len(frames) < self.max_frames:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "        return torch.stack(frames)  # Shape: (T, C, H, W)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(\"video path:\", base_vid_path + self.df.loc[idx, video_path_col])\n",
    "        #print(\":\", base_vid_path + self.df.loc[idx, video_path_col])\n",
    "        #print(\"idx:\", idx)\n",
    "        sub_path = self.df.loc[idx, video_path_col]\n",
    "        video_tensor = self._load_video_frames(base_vid_path + sub_path)  # (T, C, H, W)\n",
    "        label = torch.tensor(self.df.loc[idx, target_col], dtype=torch.long)\n",
    "        #video_id = self.df.loc[idx, id_col]\n",
    "        #return video_tensor, label, video_id\n",
    "        return video_tensor, label, sub_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a954c908-971f-48f0-b709-e99bbcd1247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_aggregate(label_list):\n",
    "    binary = [1 if label == \"YES\" else 0 for label in label_list]\n",
    "    return Counter(binary).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d525658-9791-4740-b0dc-f3830227cced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583\n",
      "195\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "data_df = pd.read_json(base_path + file_name).T[[video_path_col, target_col, id_col]]\n",
    "X = data_df.drop(target_col, axis=1)\n",
    "y = data_df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "test_df = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "train_ids = train_df[id_col].to_list()\n",
    "test_ids = test_df[id_col].to_list()\n",
    "train_df[target_col] = train_df[target_col].apply(convert_and_aggregate)\n",
    "test_df[target_col] = test_df[target_col].apply(convert_and_aggregate)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = VideoDataset(train_df, transform)\n",
    "test_dataset = VideoDataset(test_df, transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#train_df = pd.read_csv(base_path + train_file_name)[[video_path_col, target_col, id_col]]\n",
    "train_df = pd.read_csv(base_path + train_file_name)[[video_path_col, target_col]]\n",
    "X_train = train_df.drop(target_col, axis=1)\n",
    "y_train = train_df[target_col]\n",
    "#train_ids = train_df[id_col].to_list()\n",
    "train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "#train_df[target_col] = train_df[target_col].apply(convert_and_aggregate)\n",
    "\n",
    "#val_df = pd.read_csv(base_path + val_file_name)[[video_path_col, target_col, id_col]]\n",
    "val_df = pd.read_csv(base_path + val_file_name)[[video_path_col, target_col]]\n",
    "X_val = val_df.drop(target_col, axis=1)\n",
    "y_val = val_df[target_col]\n",
    "#val_ids = val_df[id_col].to_list()\n",
    "val_df = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)\n",
    "#val_df[target_col] = val_df[target_col].apply(convert_and_aggregate)\n",
    "\n",
    "#test_df = pd.read_csv(base_path + test_file_name)[[video_path_col, target_col, id_col]]\n",
    "test_df = pd.read_csv(base_path + test_file_name)[[video_path_col, target_col]]\n",
    "X_test = test_df.drop(target_col, axis=1)\n",
    "y_test = test_df[target_col]\n",
    "#test_ids = test_df[id_col].to_list()\n",
    "test_df = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "#test_df[target_col] = test_df[target_col].apply(convert_and_aggregate)\n",
    "\n",
    "train_dataset = VideoDataset(train_df, transform)\n",
    "val_dataset = VideoDataset(val_df, transform)\n",
    "test_dataset = VideoDataset(test_df, transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301c37f5-c1c9-4972-a8c1-5c77d13be0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ../../../../dat/EXIST_2025_Videos_Dataset/train_test/train_df.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count              583\n",
       "unique              13\n",
       "top       ['NO', 'NO']\n",
       "freq               233\n",
       "Name: labels_task3_1, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_df = pd.read_json(base_path+file_name).T\n",
    "#data_df.describe()\n",
    "print(\"path:\", base_path+train_file_name)\n",
    "data_df = pd.read_csv(base_path+train_file_name)['labels_task3_1']\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b4830a-43aa-4e87-b7ce-d9f9601a671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, bottleneck_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(bottleneck_channels, bottleneck_channels * self.expansion,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(bottleneck_channels * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4143e7-19f6-439d-a722-a1af3b1f4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, bottleneck_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        out_channels = bottleneck_channels * block.expansion\n",
    "\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = [block(self.in_channels, bottleneck_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, bottleneck_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4630b5ab-c58b-4fb6-95e4-84ce2532b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFeatureExtractor(ResNet):\n",
    "    def __init__(self, block, layers):\n",
    "        super().__init__(block, layers)\n",
    "        self.fc = nn.Identity()  # Remove classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x  # Features instead of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0909a5f-9132-4e33-8cc7-8a27f1eab138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet101_features():\n",
    "    return ResNetFeatureExtractor(Bottleneck, [3, 4, 23, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3a95ec-748a-4bc7-8878-3be3aed81af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationModel(nn.Module):\n",
    "    def __init__(self, feature_dim=2048, hidden_dim=512, num_layers=1, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.cnn = resnet101_features()\n",
    "        self.lstm = nn.LSTM(input_size=feature_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, video_frames):\n",
    "        # video_frames: (batch_size, seq_len, C, H, W)\n",
    "        batch_size, seq_len, C, H, W = video_frames.shape\n",
    "        features = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            frame = video_frames[:, t, :, :, :]  # (B, C, H, W)\n",
    "            feature = self.cnn(frame)            # (B, 2048)\n",
    "            features.append(feature)\n",
    "\n",
    "        features = torch.stack(features, dim=1)  # (B, T, 2048)\n",
    "        lstm_out, _ = self.lstm(features)        # (B, T, hidden_dim)\n",
    "        last_hidden = lstm_out[:, -1, :]         # (B, hidden_dim)\n",
    "        logits = self.classifier(last_hidden)    # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d424b9-df92-4b43-98a2-59badceb1505",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8462d1d-1d4f-4895-b66a-d6a9b3818e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████| 73/73 [01:27<00:00,  1.19s/it, loss=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.7070, Accuracy: 0.5437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████████████| 73/73 [01:27<00:00,  1.19s/it, loss=0.688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.6858, Accuracy: 0.5798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████| 73/73 [01:26<00:00,  1.19s/it, loss=0.638]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.6803, Accuracy: 0.5660\n",
      "elapsed time: 0:04:21.431986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "model = VideoClassificationModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 3\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_preds, train_probs, train_targets, train_paths = [], [], [], []\n",
    "#train_preds, train_probs, train_targets, train_ids = [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    #for videos, labels, video_ids in loop:\n",
    "    for videos, labels, paths in loop:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        train_preds.extend(preds.detach().cpu().numpy())\n",
    "        train_probs.extend(probs.detach().cpu().numpy())\n",
    "        train_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        #train_ids.extend(video_ids)\n",
    "        train_paths.extend(paths)\n",
    "        \n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"elapsed time:\", end_time - start_time)\n",
    "\n",
    "probs_df = pd.DataFrame(train_probs, columns=[\"prob_class_0\", \"prob_class_1\"])\n",
    "probs_df[\"predicted_label\"] = train_preds\n",
    "probs_df[\"true_label\"] = train_targets\n",
    "probs_df[\"path\"] = train_paths\n",
    "#probs_df[id_col] = train_ids\n",
    "probs_df.to_csv(\"resnet101_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce2f1f-77c3-43af-b190-a331662445d3",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb72f5e0-a803-41b5-949b-4b95002e2a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:19<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_preds, val_probs, val_targets, val_paths = [], [], [], []\n",
    "#train_preds, train_probs, train_targets, train_ids = [], [], [], []\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    #for videos, labels, video_ids in loop:\n",
    "    for videos, labels, paths in loop:\n",
    "        videos = videos.to(device)\n",
    "        outputs = model(videos)\n",
    "    \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        val_preds.extend(preds.detach().cpu().numpy())\n",
    "        val_probs.extend(probs.detach().cpu().numpy())\n",
    "        val_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        #train_ids.extend(video_ids)\n",
    "        val_paths.extend(paths)\n",
    "\n",
    "val_acc = accuracy_score(val_targets, val_preds)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "probs_df = pd.DataFrame(val_probs, columns=[\"prob_class_0\", \"prob_class_1\"])\n",
    "probs_df[\"predicted_label\"] = val_preds\n",
    "probs_df[\"true_label\"] = val_targets\n",
    "probs_df[\"path\"] = val_paths\n",
    "#probs_df[id_col] = train_ids\n",
    "probs_df.to_csv(\"resnet101_validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee444b6-f54a-4356-a184-e38d167c4a48",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9e16f24-e263-4741-83ec-7341b806adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:19<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_preds, test_probs, test_targets, test_paths = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for videos, labels, paths in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        videos = videos.to(device)\n",
    "        outputs = model(videos)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "        test_targets.extend(labels.numpy())\n",
    "\n",
    "        #test_ids.extend(video_ids)\n",
    "        test_paths.extend(paths)\n",
    "\n",
    "acc = accuracy_score(test_targets, test_preds)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "probs_df = pd.DataFrame(test_probs, columns=[\"prob_class_0\", \"prob_class_1\"])\n",
    "probs_df[\"predicted_label\"] = test_preds\n",
    "probs_df[\"true_label\"] = test_targets\n",
    "probs_df[\"path\"] = test_paths\n",
    "#probs_df[id_col] = test_ids\n",
    "probs_df.to_csv(\"resnet101_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eed7fb-e722-422c-8477-522c8068a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5c301-43fe-4a64-b15f-3779d9a2cbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a1c50-6489-4b04-b8c6-70100536c528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b66abe-344f-4335-b485-6487526af2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266076ae-a74d-4523-a896-b7040ffc05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train label distribution:\", train_df[target_col].value_counts())\n",
    "print(\"Val label distribution:\", val_df[target_col].value_counts())\n",
    "print(\"Test label distribution:\", test_df[target_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c19b2-a394-46da-a9e8-6e0a20c8fd49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

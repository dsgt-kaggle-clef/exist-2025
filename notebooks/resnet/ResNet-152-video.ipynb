{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6edd9e-902f-4cb2-8b29-8fda485e4309",
   "metadata": {},
   "source": [
    "# ResNet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea1e14f-6151-4a72-ab2b-c707244742ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd4f8bb-e943-4706-87c9-1189e9444f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eaf6536-3832-4546-96e0-6f9a4c87ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the path for testing on my local machine\n",
    "base_path = '../../../../dat/EXIST_2025_Videos_Dataset/training/'\n",
    "\n",
    "# This is the path for running on the PACE Cluster\n",
    "#base_path = '/storage/coda1/p-dsgt_clef2025/0/shared/exist/latest/EXIST 2025 Memes Dataset/training/'\n",
    "\n",
    "file_name = 'EXIST2025_training.json'\n",
    "\n",
    "#print(\"path:\", train_path)\n",
    "video_path_col = 'path_video'\n",
    "target_col = 'labels_task3_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53a0d49-5bff-4f6e-b5a3-aa6ef5181433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, df, transform, max_frames=16):\n",
    "        #self.video_paths = video_paths\n",
    "        #self.labels = labels\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        print(len(self.df))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_video_frames(self, video_path):\n",
    "        import cv2\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        success, frame = cap.read()\n",
    "        while success and len(frames) < self.max_frames:\n",
    "            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frames.append(self.transform(img))\n",
    "            success, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        # Pad with zeros if too short\n",
    "        while len(frames) < self.max_frames:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "        return torch.stack(frames)  # Shape: (T, C, H, W)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_tensor = self._load_video_frames(base_path + self.df.loc[idx, video_path_col])  # (T, C, H, W)\n",
    "        label = torch.tensor(self.df.loc[idx, target_col], dtype=torch.long)\n",
    "        return video_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a954c908-971f-48f0-b709-e99bbcd1247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_aggregate(label_list):\n",
    "    binary = [1 if label == \"YES\" else 0 for label in label_list]\n",
    "    return Counter(binary).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d525658-9791-4740-b0dc-f3830227cced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n",
      "505\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_df = pd.read_json(base_path+file_name).T[[video_path_col, target_col]]\n",
    "X = data_df.drop(target_col, axis=1)\n",
    "y = data_df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#train_df = pd.concat([X_train, y_train], axis=1)\n",
    "#test_df = pd.concat([X_test, y_test], axis=1)\n",
    "train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "test_df = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "train_df[target_col] = train_df[target_col].apply(convert_and_aggregate)\n",
    "test_df[target_col] = test_df[target_col].apply(convert_and_aggregate)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = VideoDataset(train_df, transform)\n",
    "test_dataset = VideoDataset(test_df, transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f6bd3-8808-4015-be56-058f722c7f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301c37f5-c1c9-4972-a8c1-5c77d13be0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_Tiktok</th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>video</th>\n",
       "      <th>path_video</th>\n",
       "      <th>url</th>\n",
       "      <th>annotators</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>labels_task3_1</th>\n",
       "      <th>labels_task3_2</th>\n",
       "      <th>labels_task3_3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>2524</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>292</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>7218429008445312261</td>\n",
       "      <td>121524</td>\n",
       "      <td>es</td>\n",
       "      <td>palabras que duelen  broken_heart  femach 8766...</td>\n",
       "      <td>7218429008445312261.mp4</td>\n",
       "      <td>videos/7218429008445312261.mp4</td>\n",
       "      <td>https://www.tiktok.com/@el_temach_3766/video/7...</td>\n",
       "      <td>[Annotator_4, Annotator_8]</td>\n",
       "      <td>2</td>\n",
       "      <td>[F, M]</td>\n",
       "      <td>[NO, NO]</td>\n",
       "      <td>[-, -]</td>\n",
       "      <td>[[-], [-]]</td>\n",
       "      <td>TRAIN-VIDEO_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1524</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>572</td>\n",
       "      <td>1780</td>\n",
       "      <td>1438</td>\n",
       "      <td>1069</td>\n",
       "      <td>1069</td>\n",
       "      <td>1069</td>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id_Tiktok id_EXIST  lang  \\\n",
       "count                  2524     2524  2524   \n",
       "unique                 2524     2524     2   \n",
       "top     7218429008445312261   121524    es   \n",
       "freq                      1        1  1524   \n",
       "\n",
       "                                                     text  \\\n",
       "count                                                2524   \n",
       "unique                                               2524   \n",
       "top     palabras que duelen  broken_heart  femach 8766...   \n",
       "freq                                                    1   \n",
       "\n",
       "                          video                      path_video  \\\n",
       "count                      2524                            2524   \n",
       "unique                     2524                            2524   \n",
       "top     7218429008445312261.mp4  videos/7218429008445312261.mp4   \n",
       "freq                          1                               1   \n",
       "\n",
       "                                                      url  \\\n",
       "count                                                2524   \n",
       "unique                                               2524   \n",
       "top     https://www.tiktok.com/@el_temach_3766/video/7...   \n",
       "freq                                                    1   \n",
       "\n",
       "                        annotators  number_annotators gender_annotators  \\\n",
       "count                         2524               2524              2524   \n",
       "unique                           8                  2                 5   \n",
       "top     [Annotator_4, Annotator_8]                  2            [F, M]   \n",
       "freq                           572               1780              1438   \n",
       "\n",
       "       labels_task3_1 labels_task3_2 labels_task3_3           split  \n",
       "count            2524           2524           2524            2524  \n",
       "unique             16             32            292               2  \n",
       "top          [NO, NO]         [-, -]     [[-], [-]]  TRAIN-VIDEO_ES  \n",
       "freq             1069           1069           1069            1524  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_json(base_path+file_name).T\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b4830a-43aa-4e87-b7ce-d9f9601a671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, bottleneck_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(bottleneck_channels, bottleneck_channels * self.expansion,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(bottleneck_channels * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4143e7-19f6-439d-a722-a1af3b1f4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, bottleneck_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        out_channels = bottleneck_channels * block.expansion\n",
    "\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = [block(self.in_channels, bottleneck_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, bottleneck_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4630b5ab-c58b-4fb6-95e4-84ce2532b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFeatureExtractor(ResNet):\n",
    "    def __init__(self, block, layers):\n",
    "        super().__init__(block, layers)\n",
    "        self.fc = nn.Identity()  # Remove classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x  # Features instead of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0909a5f-9132-4e33-8cc7-8a27f1eab138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resnet152_features():\n",
    "    return ResNetFeatureExtractor(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3a95ec-748a-4bc7-8878-3be3aed81af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationModel(nn.Module):\n",
    "    def __init__(self, feature_dim=2048, hidden_dim=512, num_layers=1, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.cnn = resnet152_features()\n",
    "        self.lstm = nn.LSTM(input_size=feature_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, video_frames):\n",
    "        # video_frames: (batch_size, seq_len, C, H, W)\n",
    "        batch_size, seq_len, C, H, W = video_frames.shape\n",
    "        features = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            frame = video_frames[:, t, :, :, :]  # (B, C, H, W)\n",
    "            feature = self.cnn(frame)            # (B, 2048)\n",
    "            features.append(feature)\n",
    "\n",
    "        features = torch.stack(features, dim=1)  # (B, T, 2048)\n",
    "        lstm_out, _ = self.lstm(features)        # (B, T, hidden_dim)\n",
    "        last_hidden = lstm_out[:, -1, :]         # (B, hidden_dim)\n",
    "        logits = self.classifier(last_hidden)    # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8462d1d-1d4f-4895-b66a-d6a9b3818e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  42%|██████████████████████████████▌                                          | 106/253 [02:03<02:50,  1.16s/it, loss=0.724][NULL @ 0x560deb790b40] Invalid NAL unit size (16738 > 4736).\n",
      "[NULL @ 0x560deb790b40] missing picture in access unit with size 4740\n",
      "[h264 @ 0x560deb715dc0] Invalid NAL unit size (16738 > 4736).\n",
      "[h264 @ 0x560deb715dc0] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560deb8ddd00] stream 1, offset 0x4bf75: partial file\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████| 253/253 [04:56<00:00,  1.17s/it, loss=0.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6997, Accuracy: 0.5196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  20%|██████████████▉                                                           | 51/253 [01:00<03:58,  1.18s/it, loss=0.824][NULL @ 0x560ddb257580] Invalid NAL unit size (16738 > 4736).\n",
      "[NULL @ 0x560ddb257580] missing picture in access unit with size 4740\n",
      "[h264 @ 0x560deba15c80] Invalid NAL unit size (16738 > 4736).\n",
      "[h264 @ 0x560deba15c80] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560deb8f0a40] stream 1, offset 0x4bf75: partial file\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████| 253/253 [04:58<00:00,  1.18s/it, loss=0.603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.6925, Accuracy: 0.5265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   6%|████▍                                                                     | 15/253 [00:17<04:43,  1.19s/it, loss=0.785][NULL @ 0x560deb7c5500] Invalid NAL unit size (16738 > 4736).\n",
      "[NULL @ 0x560deb7c5500] missing picture in access unit with size 4740\n",
      "[h264 @ 0x560dea2afc40] Invalid NAL unit size (16738 > 4736).\n",
      "[h264 @ 0x560dea2afc40] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560ddb12e740] stream 1, offset 0x4bf75: partial file\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████| 253/253 [04:57<00:00,  1.17s/it, loss=0.612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.6901, Accuracy: 0.5478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = VideoClassificationModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 3\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for videos, labels in loop:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e16f24-e263-4741-83ec-7341b806adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_preds, test_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for videos, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        videos = videos.to(device)\n",
    "        outputs = model(videos)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_targets.extend(labels.numpy())\n",
    "\n",
    "acc = accuracy_score(test_targets, test_preds)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eed7fb-e722-422c-8477-522c8068a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5c301-43fe-4a64-b15f-3779d9a2cbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a1c50-6489-4b04-b8c6-70100536c528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "437ffc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0f57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e69b46ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf82a7",
   "metadata": {},
   "source": [
    "## Get the disagreement exist ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4e8d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(809, 7)\n",
      "Shape of merged dataframe: (774, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>description_trial1</th>\n",
       "      <th>label_trial1</th>\n",
       "      <th>path_video</th>\n",
       "      <th>analysis_trial1</th>\n",
       "      <th>description_trial3</th>\n",
       "      <th>label_trial3</th>\n",
       "      <th>analysis_trial3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220242</td>\n",
       "      <td>these men stay  sparkles pressed sparkles  che...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A woman in a TikTok video recounts an interact...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/6920327322679692545.mp4</td>\n",
       "      <td>The video criticizes sexism by highlighting an...</td>\n",
       "      <td>The video features a woman recounting and sati...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video depicts sexist ideas attributed to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220296</td>\n",
       "      <td>beaming_face_with_smiling_eyes  ‘school’ pent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The video contrasts Google image search result...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/6935046770778967302.mp4</td>\n",
       "      <td>The video criticizes sexism by highlighting th...</td>\n",
       "      <td>The video contrasts Google Images search resul...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video itself does not promote or endorse s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220920</td>\n",
       "      <td>don’t laugh at your girlfriends choice, you ar...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A woman tells a joke: 'You should never laugh ...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7123342389338443009.mp4</td>\n",
       "      <td>The joke implies that women make poor choices,...</td>\n",
       "      <td>The video features a woman telling a joke advi...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video presents a lighthearted joke about r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220665</td>\n",
       "      <td>women, why do we do this?  woman_facepalming_m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A woman on a bed intentionally shows she's ups...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7039365996053794053.mp4</td>\n",
       "      <td>The video embodies sexism by humorously reinfo...</td>\n",
       "      <td>The video humorously depicts a common relation...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video uses gender stereotypes for comedy b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220746</td>\n",
       "      <td>the struggle is real is caffeinated.  most men...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A woman with purple hair and dark lipstick des...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7067715837255470383.mp4</td>\n",
       "      <td>The video embodies sexism by making a broad ge...</td>\n",
       "      <td>A woman humorously describes the difficulty of...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video makes a lighthearted observation abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_EXIST                                               text  target  \\\n",
       "0    220242  these men stay  sparkles pressed sparkles  che...     1.0   \n",
       "1    220296   beaming_face_with_smiling_eyes  ‘school’ pent...     1.0   \n",
       "2    220920  don’t laugh at your girlfriends choice, you ar...     0.0   \n",
       "3    220665  women, why do we do this?  woman_facepalming_m...     1.0   \n",
       "4    220746  the struggle is real is caffeinated.  most men...     0.0   \n",
       "\n",
       "                                  description_trial1 label_trial1  \\\n",
       "0  A woman in a TikTok video recounts an interact...          YES   \n",
       "1  The video contrasts Google image search result...          YES   \n",
       "2  A woman tells a joke: 'You should never laugh ...          YES   \n",
       "3  A woman on a bed intentionally shows she's ups...          YES   \n",
       "4  A woman with purple hair and dark lipstick des...          YES   \n",
       "\n",
       "                       path_video  \\\n",
       "0  videos/6920327322679692545.mp4   \n",
       "1  videos/6935046770778967302.mp4   \n",
       "2  videos/7123342389338443009.mp4   \n",
       "3  videos/7039365996053794053.mp4   \n",
       "4  videos/7067715837255470383.mp4   \n",
       "\n",
       "                                     analysis_trial1  \\\n",
       "0  The video criticizes sexism by highlighting an...   \n",
       "1  The video criticizes sexism by highlighting th...   \n",
       "2  The joke implies that women make poor choices,...   \n",
       "3  The video embodies sexism by humorously reinfo...   \n",
       "4  The video embodies sexism by making a broad ge...   \n",
       "\n",
       "                                  description_trial3 label_trial3  \\\n",
       "0  The video features a woman recounting and sati...           NO   \n",
       "1  The video contrasts Google Images search resul...           NO   \n",
       "2  The video features a woman telling a joke advi...           NO   \n",
       "3  The video humorously depicts a common relation...           NO   \n",
       "4  A woman humorously describes the difficulty of...           NO   \n",
       "\n",
       "                                     analysis_trial3  \n",
       "0  The video depicts sexist ideas attributed to a...  \n",
       "1  The video itself does not promote or endorse s...  \n",
       "2  The video presents a lighthearted joke about r...  \n",
       "3  The video uses gender stereotypes for comedy b...  \n",
       "4  The video makes a lighthearted observation abo...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the subset data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "subset_folder = \"subsets_trial5\"\n",
    "csv_files = [f for f in os.listdir(subset_folder) if f.endswith('.csv')]\n",
    "\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(subset_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"⚠️ UTF-8 failed for {file}, trying ISO-8859-1...\")\n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "df_all = df_all.loc[:, ~df_all.columns.str.startswith('Unnamed:')]\n",
    "df_all = df_all[df_all['description'] != 'ERROR']\n",
    "df_all['id_EXIST'] = df_all['id_EXIST'].astype('Int64')\n",
    "df_all = df_all[df_all['id_EXIST'].notna()]\n",
    "\n",
    "df_trial1 = df_all\n",
    "\n",
    "print(df_trial1.shape)\n",
    "df_trial1.head()\n",
    "\n",
    "subset_folder = \"subsets_trial3\"\n",
    "csv_files = [f for f in os.listdir(subset_folder) if f.endswith('.csv')]\n",
    "\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(subset_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"⚠️ UTF-8 failed for {file}, trying ISO-8859-1...\")\n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "df_all = df_all.loc[:, ~df_all.columns.str.startswith('Unnamed:')]\n",
    "df_all = df_all[df_all['description'] != 'ERROR']\n",
    "df_all['id_EXIST'] = df_all['id_EXIST'].astype('Int64')\n",
    "df_all = df_all[df_all['id_EXIST'].notna()]\n",
    "\n",
    "df_trial3 = df_all\n",
    "df_trial3.head()\n",
    "# Merge the two dataframes on specified columns\n",
    "merged_df = pd.merge(\n",
    "    df_trial1[['id_EXIST', 'text', 'target', 'description','label','path_video','analysis']], \n",
    "    df_trial3[['id_EXIST', 'description','label','analysis']],\n",
    "    on='id_EXIST',\n",
    "    suffixes=('_trial1', '_trial3')\n",
    ")\n",
    "\n",
    "print(\"Shape of merged dataframe:\", merged_df.shape)\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76938fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of disagreement cases: (338, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>description_trial1</th>\n",
       "      <th>label_trial1</th>\n",
       "      <th>path_video</th>\n",
       "      <th>analysis_trial1</th>\n",
       "      <th>description_trial3</th>\n",
       "      <th>label_trial3</th>\n",
       "      <th>analysis_trial3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220242</td>\n",
       "      <td>these men stay  sparkles pressed sparkles  che...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A woman in a TikTok video recounts an interact...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/6920327322679692545.mp4</td>\n",
       "      <td>The video criticizes sexism by highlighting an...</td>\n",
       "      <td>The video features a woman recounting and sati...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video depicts sexist ideas attributed to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220296</td>\n",
       "      <td>beaming_face_with_smiling_eyes  ‘school’ pent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The video contrasts Google image search result...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/6935046770778967302.mp4</td>\n",
       "      <td>The video criticizes sexism by highlighting th...</td>\n",
       "      <td>The video contrasts Google Images search resul...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video itself does not promote or endorse s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220920</td>\n",
       "      <td>don’t laugh at your girlfriends choice, you ar...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A woman tells a joke: 'You should never laugh ...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7123342389338443009.mp4</td>\n",
       "      <td>The joke implies that women make poor choices,...</td>\n",
       "      <td>The video features a woman telling a joke advi...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video presents a lighthearted joke about r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220665</td>\n",
       "      <td>women, why do we do this?  woman_facepalming_m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A woman on a bed intentionally shows she's ups...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7039365996053794053.mp4</td>\n",
       "      <td>The video embodies sexism by humorously reinfo...</td>\n",
       "      <td>The video humorously depicts a common relation...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video uses gender stereotypes for comedy b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220746</td>\n",
       "      <td>the struggle is real is caffeinated.  most men...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A woman with purple hair and dark lipstick des...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7067715837255470383.mp4</td>\n",
       "      <td>The video embodies sexism by making a broad ge...</td>\n",
       "      <td>A woman humorously describes the difficulty of...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video makes a lighthearted observation abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_EXIST                                               text  target  \\\n",
       "0    220242  these men stay  sparkles pressed sparkles  che...     1.0   \n",
       "1    220296   beaming_face_with_smiling_eyes  ‘school’ pent...     1.0   \n",
       "2    220920  don’t laugh at your girlfriends choice, you ar...     0.0   \n",
       "3    220665  women, why do we do this?  woman_facepalming_m...     1.0   \n",
       "4    220746  the struggle is real is caffeinated.  most men...     0.0   \n",
       "\n",
       "                                  description_trial1 label_trial1  \\\n",
       "0  A woman in a TikTok video recounts an interact...          YES   \n",
       "1  The video contrasts Google image search result...          YES   \n",
       "2  A woman tells a joke: 'You should never laugh ...          YES   \n",
       "3  A woman on a bed intentionally shows she's ups...          YES   \n",
       "4  A woman with purple hair and dark lipstick des...          YES   \n",
       "\n",
       "                       path_video  \\\n",
       "0  videos/6920327322679692545.mp4   \n",
       "1  videos/6935046770778967302.mp4   \n",
       "2  videos/7123342389338443009.mp4   \n",
       "3  videos/7039365996053794053.mp4   \n",
       "4  videos/7067715837255470383.mp4   \n",
       "\n",
       "                                     analysis_trial1  \\\n",
       "0  The video criticizes sexism by highlighting an...   \n",
       "1  The video criticizes sexism by highlighting th...   \n",
       "2  The joke implies that women make poor choices,...   \n",
       "3  The video embodies sexism by humorously reinfo...   \n",
       "4  The video embodies sexism by making a broad ge...   \n",
       "\n",
       "                                  description_trial3 label_trial3  \\\n",
       "0  The video features a woman recounting and sati...           NO   \n",
       "1  The video contrasts Google Images search resul...           NO   \n",
       "2  The video features a woman telling a joke advi...           NO   \n",
       "3  The video humorously depicts a common relation...           NO   \n",
       "4  A woman humorously describes the difficulty of...           NO   \n",
       "\n",
       "                                     analysis_trial3  \n",
       "0  The video depicts sexist ideas attributed to a...  \n",
       "1  The video itself does not promote or endorse s...  \n",
       "2  The video presents a lighthearted joke about r...  \n",
       "3  The video uses gender stereotypes for comedy b...  \n",
       "4  The video makes a lighthearted observation abo...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for disagreements between trial1 and trial3\n",
    "disagreement_df = merged_df[merged_df['label_trial1'] != merged_df['label_trial3']]\n",
    "print(\"Shape of disagreement cases:\", disagreement_df.shape)\n",
    "disagreement_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae42e63",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22934811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2524, 14)\n",
      "(1000, 14)\n"
     ]
    }
   ],
   "source": [
    "# import libraries pandas matplotlib and seaborn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "data_path = '/Users/moiz.ali/Downloads/EXIST 2025 Dataset V0.3/EXIST 2025 Videos Dataset/training/EXIST2025_training.json'\n",
    "# Normalize the nested JSON into a DataFrame\n",
    "df = pd.read_json(data_path).T\n",
    "\n",
    "print(df.shape)\n",
    "# Display the DataFrame\n",
    "df.head()\n",
    "# Select only the en \n",
    "df = df[df['lang'] == 'en']\n",
    "\n",
    "print(df.shape)\n",
    "# Display the DataFrame\n",
    "df.head()\n",
    "# Creating binary label based on majority vote of labels_task3_1\n",
    "\n",
    "def majority_vote(lst):\n",
    "    return pd.Series(lst).mode().iloc[0]\n",
    "\n",
    "df['target'] = df['labels_task3_1'].apply(majority_vote)\n",
    "df['target'] = df['target'].map({'YES': 1, 'NO': 0})\n",
    "\n",
    "df.head()\n",
    "# Train Test Split\n",
    "# remove the ones with bad videos\n",
    "bad_vids_df = pd.read_csv(\"/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/bad_videos/bad_videos_log.csv\")\n",
    "bad_vids_df.head()\n",
    "clean_df = df[~df['video'].isin(bad_vids_df['filename'])]\n",
    "clean_df.shape\n",
    "df = clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "444325fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>description_trial1</th>\n",
       "      <th>label_trial1</th>\n",
       "      <th>path_video</th>\n",
       "      <th>analysis_trial1</th>\n",
       "      <th>description_trial3</th>\n",
       "      <th>label_trial3</th>\n",
       "      <th>analysis_trial3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220242</td>\n",
       "      <td>these men stay  sparkles pressed sparkles  che...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A woman in a TikTok video recounts an interact...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/6920327322679692545.mp4</td>\n",
       "      <td>The video criticizes sexism by highlighting an...</td>\n",
       "      <td>The video features a woman recounting and sati...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video depicts sexist ideas attributed to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220296</td>\n",
       "      <td>beaming_face_with_smiling_eyes  ‘school’ pent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The video contrasts Google image search result...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/6935046770778967302.mp4</td>\n",
       "      <td>The video criticizes sexism by highlighting th...</td>\n",
       "      <td>The video contrasts Google Images search resul...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video itself does not promote or endorse s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220920</td>\n",
       "      <td>don’t laugh at your girlfriends choice, you ar...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A woman tells a joke: 'You should never laugh ...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7123342389338443009.mp4</td>\n",
       "      <td>The joke implies that women make poor choices,...</td>\n",
       "      <td>The video features a woman telling a joke advi...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video presents a lighthearted joke about r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220665</td>\n",
       "      <td>women, why do we do this?  woman_facepalming_m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A woman on a bed intentionally shows she's ups...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7039365996053794053.mp4</td>\n",
       "      <td>The video embodies sexism by humorously reinfo...</td>\n",
       "      <td>The video humorously depicts a common relation...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video uses gender stereotypes for comedy b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220746</td>\n",
       "      <td>the struggle is real is caffeinated.  most men...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A woman with purple hair and dark lipstick des...</td>\n",
       "      <td>YES</td>\n",
       "      <td>videos/7067715837255470383.mp4</td>\n",
       "      <td>The video embodies sexism by making a broad ge...</td>\n",
       "      <td>A woman humorously describes the difficulty of...</td>\n",
       "      <td>NO</td>\n",
       "      <td>The video makes a lighthearted observation abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_EXIST                                               text  target  \\\n",
       "0    220242  these men stay  sparkles pressed sparkles  che...     1.0   \n",
       "1    220296   beaming_face_with_smiling_eyes  ‘school’ pent...     1.0   \n",
       "2    220920  don’t laugh at your girlfriends choice, you ar...     0.0   \n",
       "3    220665  women, why do we do this?  woman_facepalming_m...     1.0   \n",
       "4    220746  the struggle is real is caffeinated.  most men...     0.0   \n",
       "\n",
       "                                  description_trial1 label_trial1  \\\n",
       "0  A woman in a TikTok video recounts an interact...          YES   \n",
       "1  The video contrasts Google image search result...          YES   \n",
       "2  A woman tells a joke: 'You should never laugh ...          YES   \n",
       "3  A woman on a bed intentionally shows she's ups...          YES   \n",
       "4  A woman with purple hair and dark lipstick des...          YES   \n",
       "\n",
       "                       path_video  \\\n",
       "0  videos/6920327322679692545.mp4   \n",
       "1  videos/6935046770778967302.mp4   \n",
       "2  videos/7123342389338443009.mp4   \n",
       "3  videos/7039365996053794053.mp4   \n",
       "4  videos/7067715837255470383.mp4   \n",
       "\n",
       "                                     analysis_trial1  \\\n",
       "0  The video criticizes sexism by highlighting an...   \n",
       "1  The video criticizes sexism by highlighting th...   \n",
       "2  The joke implies that women make poor choices,...   \n",
       "3  The video embodies sexism by humorously reinfo...   \n",
       "4  The video embodies sexism by making a broad ge...   \n",
       "\n",
       "                                  description_trial3 label_trial3  \\\n",
       "0  The video features a woman recounting and sati...           NO   \n",
       "1  The video contrasts Google Images search resul...           NO   \n",
       "2  The video features a woman telling a joke advi...           NO   \n",
       "3  The video humorously depicts a common relation...           NO   \n",
       "4  A woman humorously describes the difficulty of...           NO   \n",
       "\n",
       "                                     analysis_trial3  \n",
       "0  The video depicts sexist ideas attributed to a...  \n",
       "1  The video itself does not promote or endorse s...  \n",
       "2  The video presents a lighthearted joke about r...  \n",
       "3  The video uses gender stereotypes for comedy b...  \n",
       "4  The video makes a lighthearted observation abo...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad4c9735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_Tiktok</th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>video</th>\n",
       "      <th>path_video</th>\n",
       "      <th>url</th>\n",
       "      <th>annotators</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>labels_task3_1</th>\n",
       "      <th>labels_task3_2</th>\n",
       "      <th>labels_task3_3</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6605282293726579974</td>\n",
       "      <td>220001</td>\n",
       "      <td>en</td>\n",
       "      <td>6605282293726579974.mp4</td>\n",
       "      <td>videos/6605282293726579974.mp4</td>\n",
       "      <td>https://www.tiktok.com/@sweetheartkittens/vide...</td>\n",
       "      <td>[Annotator_1, Annotator_5]</td>\n",
       "      <td>2</td>\n",
       "      <td>[F, M]</td>\n",
       "      <td>[NO, NO]</td>\n",
       "      <td>[-, -]</td>\n",
       "      <td>[[-], [-]]</td>\n",
       "      <td>TRAIN-VIDEO_EN</td>\n",
       "      <td>0</td>\n",
       "      <td>The video features a kitten being petted. Ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6630477888766348549</td>\n",
       "      <td>220002</td>\n",
       "      <td>en</td>\n",
       "      <td>6630477888766348549.mp4</td>\n",
       "      <td>videos/6630477888766348549.mp4</td>\n",
       "      <td>https://www.tiktok.com/@katjaglieson/video/663...</td>\n",
       "      <td>[Annotator_1, Annotator_5, Annotator_10]</td>\n",
       "      <td>3</td>\n",
       "      <td>[F, M, F]</td>\n",
       "      <td>[NO, YES, YES]</td>\n",
       "      <td>[-, DIRECT, JUDGEMENTAL]</td>\n",
       "      <td>[[-], [OBJECTIFICATION], [OBJECTIFICATION]]</td>\n",
       "      <td>TRAIN-VIDEO_EN</td>\n",
       "      <td>1</td>\n",
       "      <td>The video criticizes sexism by portraying catc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6724072000509185286</td>\n",
       "      <td>220004</td>\n",
       "      <td>en</td>\n",
       "      <td>6724072000509185286.mp4</td>\n",
       "      <td>videos/6724072000509185286.mp4</td>\n",
       "      <td>https://www.tiktok.com/@spazzyy00/video/672407...</td>\n",
       "      <td>[Annotator_2, Annotator_6]</td>\n",
       "      <td>2</td>\n",
       "      <td>[F, F]</td>\n",
       "      <td>[NO, NO]</td>\n",
       "      <td>[-, -]</td>\n",
       "      <td>[[-], [-]]</td>\n",
       "      <td>TRAIN-VIDEO_EN</td>\n",
       "      <td>0</td>\n",
       "      <td>The video embodies sexism by perpetuating the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6739312750398409990</td>\n",
       "      <td>220005</td>\n",
       "      <td>en</td>\n",
       "      <td>6739312750398409990.mp4</td>\n",
       "      <td>videos/6739312750398409990.mp4</td>\n",
       "      <td>https://www.tiktok.com/@naliuh/video/673931275...</td>\n",
       "      <td>[Annotator_2, Annotator_6, Annotator_10]</td>\n",
       "      <td>3</td>\n",
       "      <td>[F, F, F]</td>\n",
       "      <td>[YES, NO, NO]</td>\n",
       "      <td>[DIRECT, -, -]</td>\n",
       "      <td>[[OBJECTIFICATION], [-], [-]]</td>\n",
       "      <td>TRAIN-VIDEO_EN</td>\n",
       "      <td>0</td>\n",
       "      <td>The video utilizes the \"Hot Mom\" stereotype, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6751959446093647110</td>\n",
       "      <td>220006</td>\n",
       "      <td>en</td>\n",
       "      <td>6751959446093647110.mp4</td>\n",
       "      <td>videos/6751959446093647110.mp4</td>\n",
       "      <td>https://www.tiktok.com/@cringecarter/video/675...</td>\n",
       "      <td>[Annotator_1, Annotator_5]</td>\n",
       "      <td>2</td>\n",
       "      <td>[F, M]</td>\n",
       "      <td>[NO, NO]</td>\n",
       "      <td>[-, -]</td>\n",
       "      <td>[[-], [-]]</td>\n",
       "      <td>TRAIN-VIDEO_EN</td>\n",
       "      <td>0</td>\n",
       "      <td>The video's humor stems from the driver's mist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id_Tiktok  id_EXIST lang                    video  \\\n",
       "0  6605282293726579974    220001   en  6605282293726579974.mp4   \n",
       "1  6630477888766348549    220002   en  6630477888766348549.mp4   \n",
       "2  6724072000509185286    220004   en  6724072000509185286.mp4   \n",
       "3  6739312750398409990    220005   en  6739312750398409990.mp4   \n",
       "4  6751959446093647110    220006   en  6751959446093647110.mp4   \n",
       "\n",
       "                       path_video  \\\n",
       "0  videos/6605282293726579974.mp4   \n",
       "1  videos/6630477888766348549.mp4   \n",
       "2  videos/6724072000509185286.mp4   \n",
       "3  videos/6739312750398409990.mp4   \n",
       "4  videos/6751959446093647110.mp4   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.tiktok.com/@sweetheartkittens/vide...   \n",
       "1  https://www.tiktok.com/@katjaglieson/video/663...   \n",
       "2  https://www.tiktok.com/@spazzyy00/video/672407...   \n",
       "3  https://www.tiktok.com/@naliuh/video/673931275...   \n",
       "4  https://www.tiktok.com/@cringecarter/video/675...   \n",
       "\n",
       "                                 annotators number_annotators  \\\n",
       "0                [Annotator_1, Annotator_5]                 2   \n",
       "1  [Annotator_1, Annotator_5, Annotator_10]                 3   \n",
       "2                [Annotator_2, Annotator_6]                 2   \n",
       "3  [Annotator_2, Annotator_6, Annotator_10]                 3   \n",
       "4                [Annotator_1, Annotator_5]                 2   \n",
       "\n",
       "  gender_annotators  labels_task3_1            labels_task3_2  \\\n",
       "0            [F, M]        [NO, NO]                    [-, -]   \n",
       "1         [F, M, F]  [NO, YES, YES]  [-, DIRECT, JUDGEMENTAL]   \n",
       "2            [F, F]        [NO, NO]                    [-, -]   \n",
       "3         [F, F, F]   [YES, NO, NO]            [DIRECT, -, -]   \n",
       "4            [F, M]        [NO, NO]                    [-, -]   \n",
       "\n",
       "                                labels_task3_3           split  target  \\\n",
       "0                                   [[-], [-]]  TRAIN-VIDEO_EN       0   \n",
       "1  [[-], [OBJECTIFICATION], [OBJECTIFICATION]]  TRAIN-VIDEO_EN       1   \n",
       "2                                   [[-], [-]]  TRAIN-VIDEO_EN       0   \n",
       "3                [[OBJECTIFICATION], [-], [-]]  TRAIN-VIDEO_EN       0   \n",
       "4                                   [[-], [-]]  TRAIN-VIDEO_EN       0   \n",
       "\n",
       "                                                text  \n",
       "0  The video features a kitten being petted. Ther...  \n",
       "1  The video criticizes sexism by portraying catc...  \n",
       "2  The video embodies sexism by perpetuating the ...  \n",
       "3  The video utilizes the \"Hot Mom\" stereotype, w...  \n",
       "4  The video's humor stems from the driver's mist...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop text column first, then rename description_trial1 to text\n",
    "\n",
    "# Merge df with merged_df on id_EXIST, keeping only description_trial1 from merged_df\n",
    "# First ensure both dataframes have the same type for id_EXIST\n",
    "df['id_EXIST'] = df['id_EXIST'].astype(int)\n",
    "merged_df['id_EXIST'] = merged_df['id_EXIST'].astype(int)\n",
    "\n",
    "# Then perform the merge\n",
    "merged_df = pd.merge(df, merged_df[['id_EXIST', 'description_trial1','description_trial3','analysis_trial1','analysis_trial3']], on='id_EXIST', how='inner')\n",
    "\n",
    "df = merged_df.copy()\n",
    "# Concatenate the columns with a separator\n",
    "# df['text2'] = df['analysis_trial1'] + df['analysis_trial3'] # Mean Test Accuracy: 0.7302 ± 0.0332\n",
    "df['text2'] = df['analysis_trial1'] + df['analysis_trial3'] + df['description_trial1'] + df['description_trial3']\n",
    "df = df.drop(columns=['text'])\n",
    "df = df.rename(columns={'text2': 'text'})\n",
    "\n",
    "\n",
    "# Drop the individual columns since we've combined them\n",
    "df = df.drop(columns=['description_trial1', 'description_trial3', 'analysis_trial1', 'analysis_trial3'])\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812687c",
   "metadata": {},
   "source": [
    "# Stratified KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a345de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, video_ids, exist_ids):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.video_ids = video_ids\n",
    "        self.exist_ids = exist_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['video_ids'] = self.video_ids[idx]\n",
    "        item['exist_ids'] = self.exist_ids[idx]\n",
    "        return item\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "def get_loader(df, batch_size=16, shuffle=False):\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['target'].tolist()\n",
    "    video_ids = df['video'].tolist()\n",
    "    exist_ids = df['id_EXIST'].tolist()\n",
    "    encodings = tokenize(texts)\n",
    "    dataset = TextDataset(encodings, labels, video_ids, exist_ids)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe5c0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(train_loader, val_loader, device, num_labels=2, freeze_layers_up_to=20, epochs=6):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels)\n",
    "    \n",
    "    # Freeze embeddings and encoder layers up to `freeze_layers_up_to`\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"roberta.embeddings\"):\n",
    "            param.requires_grad = False\n",
    "        elif \"roberta.encoder.layer\" in name:\n",
    "            layer_num = int(name.split(\"layer.\")[1].split(\".\")[0])\n",
    "            if layer_num <= freeze_layers_up_to:\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Train\"):\n",
    "            batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "            inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "            outputs = model(**inputs)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (torch.argmax(logits, dim=1) == batch['labels']).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Val\"):\n",
    "                batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "                inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "                outputs = model(**inputs)\n",
    "                val_loss += outputs.loss.item()\n",
    "                correct += (torch.argmax(outputs.logits, dim=1) == batch['labels']).sum().item()\n",
    "                total += batch['labels'].size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "        print(f\"[Epoch {epoch+1}] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "460d50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(df, disagreement_df, k=5, device='cuda'):\n",
    "    df['id_EXIST'] = df['id_EXIST'].astype(int)\n",
    "    disagreement_df['id_EXIST'] = disagreement_df['id_EXIST'].astype(int)\n",
    "    \n",
    "    test_df = df[df['id_EXIST'].isin(disagreement_df['id_EXIST'])]\n",
    "    # Keep only 20% of test_df and add remaining 80% to df_train_valid\n",
    "    test_df, remaining_test = train_test_split(test_df, test_size=0.5, random_state=99)\n",
    "    print(\"Shape of test_df after keeping 20%:\", test_df.shape)\n",
    "\n",
    "    # Keep the rest as df_train_valid and add the remaining test samples\n",
    "    df_train_valid = df[~df['id_EXIST'].isin(disagreement_df['id_EXIST'])]\n",
    "    df_train_valid = pd.concat([df_train_valid, remaining_test])\n",
    "    df_train_valid = df_train_valid.reset_index(drop=True)\n",
    "    print(\"Shape of df_train_valid:\", df_train_valid.shape)\n",
    "\n",
    "    # Check that test_df and df_train_valid have no overlapping exist_ids\n",
    "    test_ids = set(test_df['id_EXIST'].unique())\n",
    "    train_valid_ids = set(df_train_valid['id_EXIST'].unique())\n",
    "    assert len(test_ids.intersection(train_valid_ids)) == 0, \"Test set and train/validation set have overlapping IDs\"\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(df_train_valid)):\n",
    "        print(f\"\\n===== Fold {fold_idx+1} =====\")\n",
    "        train_df = df_train_valid.iloc[train_idx]\n",
    "        val_df = df_train_valid.iloc[val_idx]\n",
    "\n",
    "        print(f\"Train set shape: {train_df.shape}\")\n",
    "        print(f\"Validation set shape: {val_df.shape}\")\n",
    "\n",
    "        train_loader = get_loader(train_df, shuffle=True)\n",
    "        val_loader = get_loader(val_df)\n",
    "        model = train_and_validate(train_loader, val_loader, device)\n",
    "\n",
    "        # Optional: Evaluate on the held-out test set\n",
    "        test_loader = get_loader(test_df)\n",
    "        model.eval()\n",
    "        preds, targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "                batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "                inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "                outputs = model(**inputs)\n",
    "                preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                targets.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        test_acc = accuracy_score(targets, preds)\n",
    "        print(f\"Fold {fold_idx+1} Test Accuracy: {test_acc:.4f}\")\n",
    "        fold_results.append(test_acc)\n",
    "\n",
    "    print(\"\\n=== K-Fold Summary ===\")\n",
    "    print(f\"Mean Test Accuracy: {np.mean(fold_results):.4f} ± {np.std(fold_results):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bacd6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_df after keeping 20%: (169, 15)\n",
      "Shape of df_train_valid: (605, 15)\n",
      "\n",
      "===== Fold 1 =====\n",
      "Train set shape: (484, 15)\n",
      "Validation set shape: (121, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.45s/it]\n",
      "Epoch 1 - Val: 100%|██████████| 8/8 [00:08<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.5909 | Val Acc: 0.6033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|██████████| 31/31 [00:41<00:00,  1.33s/it]\n",
      "Epoch 2 - Val: 100%|██████████| 8/8 [00:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.6281 | Val Acc: 0.6033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|██████████| 31/31 [00:41<00:00,  1.35s/it]\n",
      "Epoch 3 - Val: 100%|██████████| 8/8 [00:08<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.7789 | Val Acc: 0.8017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|██████████| 31/31 [00:41<00:00,  1.35s/it]\n",
      "Epoch 4 - Val: 100%|██████████| 8/8 [00:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.8306 | Val Acc: 0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|██████████| 31/31 [00:41<00:00,  1.34s/it]\n",
      "Epoch 5 - Val: 100%|██████████| 8/8 [00:07<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8430 | Val Acc: 0.7686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|██████████| 31/31 [00:41<00:00,  1.33s/it]\n",
      "Epoch 6 - Val: 100%|██████████| 8/8 [00:08<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Acc: 0.8492 | Val Acc: 0.7934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 11/11 [00:13<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test Accuracy: 0.7811\n",
      "\n",
      "===== Fold 2 =====\n",
      "Train set shape: (484, 15)\n",
      "Validation set shape: (121, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|██████████| 31/31 [00:52<00:00,  1.69s/it]\n",
      "Epoch 1 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.6033 | Val Acc: 0.5702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|██████████| 31/31 [00:44<00:00,  1.44s/it]\n",
      "Epoch 2 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.6921 | Val Acc: 0.6777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.46s/it]\n",
      "Epoch 3 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.7541 | Val Acc: 0.6942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.46s/it]\n",
      "Epoch 4 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.7913 | Val Acc: 0.7934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.47s/it]\n",
      "Epoch 5 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8326 | Val Acc: 0.8264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|██████████| 31/31 [00:44<00:00,  1.45s/it]\n",
      "Epoch 6 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Acc: 0.8574 | Val Acc: 0.8430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 11/11 [00:12<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Test Accuracy: 0.7396\n",
      "\n",
      "===== Fold 3 =====\n",
      "Train set shape: (484, 15)\n",
      "Validation set shape: (121, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|██████████| 31/31 [00:49<00:00,  1.60s/it]\n",
      "Epoch 1 - Val: 100%|██████████| 8/8 [00:07<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.5930 | Val Acc: 0.6116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.46s/it]\n",
      "Epoch 2 - Val: 100%|██████████| 8/8 [00:07<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.6260 | Val Acc: 0.6116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.47s/it]\n",
      "Epoch 3 - Val: 100%|██████████| 8/8 [00:07<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.7479 | Val Acc: 0.7769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.47s/it]\n",
      "Epoch 4 - Val: 100%|██████████| 8/8 [00:07<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.8099 | Val Acc: 0.7769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|██████████| 31/31 [00:46<00:00,  1.48s/it]\n",
      "Epoch 5 - Val: 100%|██████████| 8/8 [00:07<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8450 | Val Acc: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.47s/it]\n",
      "Epoch 6 - Val: 100%|██████████| 8/8 [00:07<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Acc: 0.8905 | Val Acc: 0.7769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 11/11 [00:11<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Test Accuracy: 0.7101\n",
      "\n",
      "===== Fold 4 =====\n",
      "Train set shape: (484, 15)\n",
      "Validation set shape: (121, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|██████████| 31/31 [00:52<00:00,  1.69s/it]\n",
      "Epoch 1 - Val: 100%|██████████| 8/8 [00:07<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.5888 | Val Acc: 0.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.46s/it]\n",
      "Epoch 2 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.6095 | Val Acc: 0.7769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|██████████| 31/31 [00:46<00:00,  1.49s/it]\n",
      "Epoch 3 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.7417 | Val Acc: 0.7851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.48s/it]\n",
      "Epoch 4 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.8202 | Val Acc: 0.8264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|██████████| 31/31 [00:45<00:00,  1.46s/it]\n",
      "Epoch 5 - Val: 100%|██████████| 8/8 [00:06<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8182 | Val Acc: 0.8347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train:  84%|████████▍ | 26/31 [00:39<00:07,  1.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisagreement_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mrun_kfold\u001b[39m\u001b[34m(df, disagreement_df, k, device)\u001b[39m\n\u001b[32m     32\u001b[39m train_loader = get_loader(train_df, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m val_loader = get_loader(val_df)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m model = \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Optional: Evaluate on the held-out test set\u001b[39;00m\n\u001b[32m     37\u001b[39m test_loader = get_loader(test_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_and_validate\u001b[39m\u001b[34m(train_loader, val_loader, device, num_labels, freeze_layers_up_to, epochs)\u001b[39m\n\u001b[32m     22\u001b[39m batch = {k: (v.to(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m     23\u001b[39m inputs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mvideo_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexist_ids\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m loss, logits = outputs.loss, outputs.logits\n\u001b[32m     27\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:1322\u001b[39m, in \u001b[36mRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1314\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1315\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1316\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1317\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1318\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1319\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1320\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1334\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:978\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    974\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    975\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    976\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    990\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    991\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:631\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    620\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    621\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    622\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         output_attentions,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:520\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    509\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    510\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    517\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    518\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    519\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    529\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:447\u001b[39m, in \u001b[36mRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    439\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    446\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    457\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/exist2025/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:370\u001b[39m, in \u001b[36mRobertaSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    366\u001b[39m is_causal = (\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    368\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    380\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_kfold(df, disagreement_df, k=5, device='mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b9c9b",
   "metadata": {},
   "source": [
    "### Keep test as disagreement one ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3e170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_df after filtering: (338, 15)\n",
      "Shape of test_df after keeping 20%: (169, 15)\n",
      "Shape of df_train_valid: (605, 15)\n",
      "Shape of train_df: (423, 15)\n",
      "Shape of valid_df: (182, 15)\n",
      "\n",
      "Test set sample:\n",
      "\n",
      "Train set sample:\n",
      "\n",
      "Validation set sample:\n",
      "\n",
      "Final shapes:\n",
      "Original df shape: (774, 15)\n",
      "Test df shape: (169, 15)\n",
      "Train df shape: (423, 15)\n",
      "Valid df shape: (182, 15)\n",
      "\n",
      "Proportions:\n",
      "Test set: 21.83%\n",
      "Train set: 54.65%\n",
      "Validation set: 23.51%\n"
     ]
    }
   ],
   "source": [
    "# Convert id_EXIST to same type in both dataframes\n",
    "df['id_EXIST'] = df['id_EXIST'].astype(int)\n",
    "disagreement_df['id_EXIST'] = disagreement_df['id_EXIST'].astype(int)\n",
    "\n",
    "# Filter test_df to only include rows where id_EXIST is in disagreement_df\n",
    "test_df = df[df['id_EXIST'].isin(disagreement_df['id_EXIST'])]\n",
    "print(\"Shape of test_df after filtering:\", test_df.shape)\n",
    "\n",
    "# Keep only 20% of test_df and add remaining 80% to df_train_valid\n",
    "test_df, remaining_test = train_test_split(test_df, test_size=0.5, random_state=1)\n",
    "print(\"Shape of test_df after keeping 20%:\", test_df.shape)\n",
    "\n",
    "# Keep the rest as df_train_valid and add the remaining test samples\n",
    "df_train_valid = df[~df['id_EXIST'].isin(disagreement_df['id_EXIST'])]\n",
    "df_train_valid = pd.concat([df_train_valid, remaining_test])\n",
    "print(\"Shape of df_train_valid:\", df_train_valid.shape)\n",
    "\n",
    "# Split df_train_valid into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df_train_valid, test_size=0.3, random_state=1)\n",
    "print(\"Shape of train_df:\", train_df.shape)\n",
    "print(\"Shape of valid_df:\", valid_df.shape)\n",
    "\n",
    "# Display first few rows of each split\n",
    "print(\"\\nTest set sample:\")\n",
    "test_df.head()\n",
    "print(\"\\nTrain set sample:\")\n",
    "train_df.head()\n",
    "print(\"\\nValidation set sample:\")\n",
    "valid_df.head()\n",
    "\n",
    "# Print final shapes\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(f\"Original df shape: {df.shape}\")\n",
    "print(f\"Test df shape: {test_df.shape}\")\n",
    "print(f\"Train df shape: {train_df.shape}\")\n",
    "print(f\"Valid df shape: {valid_df.shape}\")\n",
    "\n",
    "# Calculate and print proportions\n",
    "total_samples = len(df)\n",
    "print(\"\\nProportions:\")\n",
    "print(f\"Test set: {len(test_df)/total_samples:.2%}\")\n",
    "print(f\"Train set: {len(train_df)/total_samples:.2%}\")\n",
    "print(f\"Validation set: {len(valid_df)/total_samples:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194863c5",
   "metadata": {},
   "source": [
    "### Keep disagreement for train, valid and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "60ea141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split test_df into train, validation and test sets\n",
    "# train_df, temp_df = train_test_split(test_df, test_size=0.3, random_state=42)\n",
    "# valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# print(\"Shape of train_df:\", train_df.shape)\n",
    "# print(\"Shape of valid_df:\", valid_df.shape)\n",
    "# print(\"Shape of test_df:\", test_df.shape)\n",
    "\n",
    "# # Display first few rows of each split\n",
    "# print(\"\\nTrain set sample:\")\n",
    "# train_df.head()\n",
    "# print(\"\\nValidation set sample:\")\n",
    "# valid_df.head()\n",
    "# print(\"\\nTest set sample:\")\n",
    "# test_df.head()\n",
    "\n",
    "# # Print final shapes\n",
    "# print(\"\\nFinal shapes:\")\n",
    "# print(f\"Original test_df shape: {test_df.shape}\")\n",
    "# print(f\"Train df shape: {train_df.shape}\")\n",
    "# print(f\"Valid df shape: {valid_df.shape}\")\n",
    "# print(f\"Test df shape: {test_df.shape}\")\n",
    "\n",
    "# # Calculate and print proportions\n",
    "# total_test_samples = len(test_df)\n",
    "# print(\"\\nProportions:\")\n",
    "# print(f\"Train set: {len(train_df)/total_test_samples:.2%}\")\n",
    "# print(f\"Validation set: {len(valid_df)/total_test_samples:.2%}\")\n",
    "# print(f\"Test set: {len(test_df)/total_test_samples:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "86fa2867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>video</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>220941</td>\n",
       "      <td>7127603784678657285.mp4</td>\n",
       "      <td>The video criticizes sexism by highlighting an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>220969</td>\n",
       "      <td>7136115124112493830.mp4</td>\n",
       "      <td>The video promotes hiding bra straps as the 'c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>220951</td>\n",
       "      <td>7132986111026105646.mp4</td>\n",
       "      <td>The video criticizes sexism by highlighting ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>220943</td>\n",
       "      <td>7128059958972943622.mp4</td>\n",
       "      <td>The video embodies sexism as the man attribute...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>220483</td>\n",
       "      <td>6978573242046958853.mp4</td>\n",
       "      <td>The video's captions, \"When your man has skill...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>220598</td>\n",
       "      <td>7014565389002607877.mp4</td>\n",
       "      <td>The video criticizes sexism by showcasing prot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>220745</td>\n",
       "      <td>7067659840079957294.mp4</td>\n",
       "      <td>The video explicitly criticizes sexism by show...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>220212</td>\n",
       "      <td>6911699116003249409.mp4</td>\n",
       "      <td>The video embodies a scenario rooted in sexism...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>220685</td>\n",
       "      <td>7047184533287963950.mp4</td>\n",
       "      <td>The video criticizes sexist societal prejudice...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>220129</td>\n",
       "      <td>6876237829119249666.mp4</td>\n",
       "      <td>The video criticizes a common sexist argument ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_EXIST                    video  \\\n",
       "726    220941  7127603784678657285.mp4   \n",
       "750    220969  7136115124112493830.mp4   \n",
       "734    220951  7132986111026105646.mp4   \n",
       "727    220943  7128059958972943622.mp4   \n",
       "372    220483  6978573242046958853.mp4   \n",
       "..        ...                      ...   \n",
       "457    220598  7014565389002607877.mp4   \n",
       "574    220745  7067659840079957294.mp4   \n",
       "169    220212  6911699116003249409.mp4   \n",
       "529    220685  7047184533287963950.mp4   \n",
       "98     220129  6876237829119249666.mp4   \n",
       "\n",
       "                                                  text  target  \n",
       "726  The video criticizes sexism by highlighting an...       1  \n",
       "750  The video promotes hiding bra straps as the 'c...       0  \n",
       "734  The video criticizes sexism by highlighting ho...       1  \n",
       "727  The video embodies sexism as the man attribute...       0  \n",
       "372  The video's captions, \"When your man has skill...       0  \n",
       "..                                                 ...     ...  \n",
       "457  The video criticizes sexism by showcasing prot...       1  \n",
       "574  The video explicitly criticizes sexism by show...       1  \n",
       "169  The video embodies a scenario rooted in sexism...       1  \n",
       "529  The video criticizes sexist societal prejudice...       0  \n",
       "98   The video criticizes a common sexist argument ...       1  \n",
       "\n",
       "[169 rows x 4 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_path = \"/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/train_test_split/test_df.csv\"\n",
    "# val_path = \"/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/train_test_split/valid_df.csv\"\n",
    "# train_path = \"/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/train_test_split/train_df.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = train_df[['id_EXIST','video','text', 'target']]\n",
    "val_data = valid_df[['id_EXIST','video','text', 'target']]\n",
    "test_data = test_df[['id_EXIST','video','text', 'target']]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b6c691fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_texts = train_data['text'].tolist()\n",
    "train_labels =  train_data['target'].tolist()\n",
    "train_video_ids =  train_data['video'].tolist()\n",
    "train_exist_ids =  train_data['id_EXIST'].tolist()\n",
    "\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels =  test_data['target'].tolist()\n",
    "test_video_ids =  test_data['video'].tolist()\n",
    "test_exist_ids =  test_data['id_EXIST'].tolist()\n",
    "\n",
    "val_texts = val_data['text'].tolist()\n",
    "val_labels =  val_data['target'].tolist()\n",
    "val_video_ids =  val_data['video'].tolist()\n",
    "val_exist_ids =  val_data['id_EXIST'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d69f242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "test_encodings = tokenize(test_texts)\n",
    "val_encodings = tokenize(val_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7cb1fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, video_ids, exist_ids):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.video_ids = video_ids\n",
    "        self.exist_ids = exist_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['video_ids'] = self.video_ids[idx]\n",
    "        item['exist_ids'] = self.exist_ids[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels, train_video_ids, train_exist_ids)\n",
    "test_dataset = TextDataset(test_encodings, test_labels, test_video_ids, test_exist_ids)\n",
    "val_dataset = TextDataset(val_encodings, val_labels, val_video_ids, val_exist_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3afb7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "72838b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2)\n",
    "\n",
    "# Freeze embeddings and encoder layers 0–20 (i.e., first 21 layers)\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"roberta.embeddings\"):\n",
    "        param.requires_grad = False\n",
    "    elif \"roberta.encoder.layer\" in name:\n",
    "        layer_num = int(name.split(\"layer.\")[1].split(\".\")[0])\n",
    "        if layer_num < 21:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        \n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                  lr=2e-05, \n",
    "                                  weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a332fef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 27/27 [01:06<00:00,  2.46s/it, loss=0.697]\n",
      "Epoch 1 - Validation: 100%|██████████| 12/12 [00:19<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.6967, Train Acc: 0.5887 | Val Loss: 0.6596, Val Acc: 0.6209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 27/27 [01:04<00:00,  2.40s/it, loss=0.635]\n",
      "Epoch 2 - Validation: 100%|██████████| 12/12 [00:19<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.6701, Train Acc: 0.5934 | Val Loss: 0.6529, Val Acc: 0.6209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 27/27 [01:05<00:00,  2.42s/it, loss=0.693]\n",
      "Epoch 3 - Validation: 100%|██████████| 12/12 [00:19<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.6575, Train Acc: 0.6383 | Val Loss: 0.6124, Val Acc: 0.6374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|██████████| 27/27 [01:06<00:00,  2.47s/it, loss=0.404]\n",
      "Epoch 4 - Validation: 100%|██████████| 12/12 [00:20<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.5589, Train Acc: 0.7045 | Val Loss: 0.4204, Val Acc: 0.7967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training:  52%|█████▏    | 14/27 [00:35<00:31,  2.44s/it, loss=0.446]"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n",
    "    for batch in loop:\n",
    "        batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        \n",
    "        #inputs = {k: v for k, v in batch.items() if k != \"video_ids\"}\n",
    "        inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == batch['labels']).sum().item()\n",
    "        total += batch['labels'].size(0)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Validation\"):\n",
    "            batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "            #inputs = {k: v for k, v in batch.items() if k != \"video_ids\"}\n",
    "            inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == batch['labels']).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# --- Final Test Evaluation ---\n",
    "model.eval()\n",
    "test_preds, test_targets = [], []\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n",
    "        batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        #inputs = {k: v for k, v in batch.items() if k != \"video_ids\"}\n",
    "        inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_targets.extend(batch['labels'].cpu().numpy())\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = accuracy_score(test_targets, test_preds)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0abe77b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Test Evaluation: 100%|██████████| 13/13 [00:22<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.7487\n",
      "Final Test Loss: 0.5620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Final Test Evaluation ---\n",
    "model.eval()\n",
    "test_preds, test_targets = [], []\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n",
    "        batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        #inputs = {k: v for k, v in batch.items() if k != \"video_ids\"}\n",
    "        inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_targets.extend(batch['labels'].cpu().numpy())\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = accuracy_score(test_targets, test_preds)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c4375448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='o')\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy', marker='o')\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01019e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def generate_predictions(model, dataloader):\n",
    "#     model.eval()\n",
    "#     all_video_ids = []\n",
    "#     all_exist_ids = []\n",
    "#     all_probs = []\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     all_constant_column = []  # This will store the constant value \"EXIST2025\"\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc=\"Generating Predictions\"):\n",
    "#             batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "#             #inputs = {k: v for k, v in batch.items() if k != \"video_ids\"}\n",
    "#             inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits\n",
    "#             probs = F.softmax(logits, dim=1)  # Get probabilities from logits\n",
    "#             preds = torch.argmax(probs, dim=1)  # Get predicted class\n",
    "#             # Map the predictions to \"YES\" (1) or \"NO\" (0)\n",
    "#             preds_mapped = [\"YES\" if pred >=0.5 else \"NO\" for pred in preds.cpu().numpy()]\n",
    "            \n",
    "#             constant_value = \"EXIST2025\"\n",
    "#             constant_column = [constant_value] * len(preds_mapped)\n",
    "            \n",
    "#             all_exist_ids.extend(batch['exist_ids'])\n",
    "#             all_video_ids.extend(batch['video_ids'])\n",
    "#             all_probs.extend(probs.cpu().numpy())\n",
    "#             all_preds.extend(preds_mapped)\n",
    "#             all_labels.extend(batch['labels'].cpu().numpy())\n",
    "#             all_constant_column.extend(constant_column)\n",
    "\n",
    "#     return np.array(all_exist_ids), np.array(all_video_ids), np.array(all_probs), np.array(all_preds), np.array(all_labels), np.array(all_constant_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "976387f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mps\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_video_ids = []\n",
    "    all_exist_ids = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_constant_column = []  # This will store the constant value \"EXIST2025\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating Predictions\"):\n",
    "            # Move tensor inputs to device (e.g., mps, cuda, cpu)\n",
    "            batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "            \n",
    "            # Remove non-model inputs from batch\n",
    "            inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Calculate probabilities and move to CPU before numpy conversion\n",
    "            probs = F.softmax(logits, dim=1).cpu()\n",
    "            \n",
    "            # Get predicted class indices and move to CPU\n",
    "            preds = torch.argmax(probs, dim=1).cpu()\n",
    "            \n",
    "            # Map predictions to \"YES\" or \"NO\" based on predicted class (assuming class 1 = YES)\n",
    "            preds_mapped = [\"YES\" if pred.item() == 1 else \"NO\" for pred in preds]\n",
    "            \n",
    "            # Prepare constant column\n",
    "            constant_value = \"EXIST2025\"\n",
    "            constant_column = [constant_value] * len(preds_mapped)\n",
    "            \n",
    "            # Append results - ensure all tensors are moved to CPU before numpy conversion\n",
    "            # all_exist_ids.extend(batch['exist_ids'])\n",
    "            # all_video_ids.extend(batch['video_ids'])\n",
    "\n",
    "            all_exist_ids.extend(\n",
    "                [x.item() if isinstance(x, torch.Tensor) else x for x in batch['exist_ids']]\n",
    "            )\n",
    "            all_video_ids.extend(\n",
    "                [x.item() if isinstance(x, torch.Tensor) else x for x in batch['video_ids']]\n",
    "            )\n",
    "            all_probs.extend(probs.numpy())  # Already on CPU\n",
    "            all_preds.extend(preds_mapped)\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "            all_constant_column.extend(constant_column)\n",
    "\n",
    "    # Convert lists to numpy arrays, ensuring all tensors are on CPU first\n",
    "    return (\n",
    "        np.array(all_exist_ids),\n",
    "        np.array(all_video_ids),\n",
    "        np.array(all_probs),\n",
    "        np.array(all_preds),\n",
    "        np.array(all_labels),\n",
    "        np.array(all_constant_column)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aefd8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 37/37 [01:04<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.0000\n",
      "Train Macro F1 Score: 0.8805\n",
      "Saved predictions to text_train_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exist_ids, video_ids, probs, preds, labels, titles = generate_predictions(model, train_loader, device=device)\n",
    "\n",
    "test_acc = accuracy_score(labels, preds)\n",
    "\n",
    "preds_binary = [1 if p == \"YES\" else 0 for p in preds]\n",
    "test_f1 = f1_score(labels, preds_binary, average=\"macro\")\n",
    "\n",
    "print(f\"Train Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Train Macro F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"test_case\": titles,\n",
    "    \"id\": exist_ids,\n",
    "    \"value\": preds\n",
    "})\n",
    "\n",
    "df.to_csv(\"text_train_predictions.csv\", index=False)\n",
    "print(\"Saved predictions to text_train_predictions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a9c18533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 13/13 [00:23<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6881\n",
      "Test Macro F1 Score: 0.6859\n",
      "Saved predictions to text_test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exist_ids, video_ids, probs, preds, labels, titles = generate_predictions(model, test_loader, device=device)\n",
    "\n",
    "preds_binary = [1 if p == \"YES\" else 0 for p in preds]\n",
    "test_acc = accuracy_score(labels, preds_binary)\n",
    "preds_binary = [1 if p == \"YES\" else 0 for p in preds]\n",
    "test_f1 = f1_score(labels, preds_binary, average=\"macro\")\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Macro F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"test_case\": titles,\n",
    "    \"id\": exist_ids,\n",
    "    \"value\": preds\n",
    "})\n",
    "\n",
    "df.to_csv(\"text_test_predictions.csv\", index=False)\n",
    "print(\"Saved predictions to text_test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e71e051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 13/13 [00:21<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.0000\n",
      "Val Macro F1 Score: 0.6814\n",
      "Saved predictions to text_val_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exist_ids, video_ids, probs, preds, labels, titles = generate_predictions(model, val_loader,device=device)\n",
    "\n",
    "test_acc = accuracy_score(labels, preds)\n",
    "preds_binary = [1 if p == \"YES\" else 0 for p in preds]\n",
    "test_f1 = f1_score(labels, preds_binary, average=\"macro\")\n",
    "\n",
    "print(f\"Val Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Val Macro F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"test_case\": titles,\n",
    "    \"id\": exist_ids,\n",
    "    \"value\": preds\n",
    "})\n",
    "\n",
    "df.to_csv(\"text_val_predictions.csv\", index=False)\n",
    "print(\"Saved predictions to text_val_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc29c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdcabc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just the model's state dict (weights)\n",
    "torch.save(model.state_dict(), \"model_weights.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c77b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3761075/2181143520.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_weights.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model architecture first\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(\"model_weights.pt\", map_location=device))\n",
    "\n",
    "# Move to device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44cb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6923\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.6308\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.6718\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.6410\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6769\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6718\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.7128\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.7179\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.7026\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6872\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6256\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.7128\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.6513\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.6051\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6103\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5128\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.6308\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.5795\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.6923\n",
      "\n",
      "Training with parameters: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6308\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5744\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.7231\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.5692\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.6974\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6718\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6923\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.6923\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.7436\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.6821\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6667\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6974\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.7077\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.7179\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.5744\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6615\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6667\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.6821\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.7026\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.6000\n",
      "\n",
      "Training with parameters: {'learning_rate': 2e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.5487\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6923\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.5692\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.5692\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.4513\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.0, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.7282\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5692\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.7282\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.7077\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.7231\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 8, 'weight_decay': 0.01, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6462\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6564\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.5692\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.6308\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n",
      "Validation Accuracy: 0.7231\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.0, 'num_frozen_layers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 20 layers.\n",
      "Validation Accuracy: 0.6718\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5538\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 6 layers.\n",
      "Validation Accuracy: 0.7333\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 12 layers.\n",
      "Validation Accuracy: 0.5692\n",
      "\n",
      "Training with parameters: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.01, 'num_frozen_layers': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Froze first 18 layers.\n"
     ]
    }
   ],
   "source": [
    "#The code below is for search grid which was used to decide the hyperparameters values\n",
    "import itertools\n",
    "\n",
    "# Define your hyperparameter search space\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5, 2e-5, 3e-5, 2e-6],\n",
    "    \"batch_size\": [8, 16],\n",
    "    \"weight_decay\": [0.0, 0.01],\n",
    "    \"num_frozen_layers\": [0, 6, 12, 18, 20]  # NEW!\n",
    "}\n",
    "\n",
    "# Create all combinations\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "best_val_acc = 0\n",
    "best_params = None\n",
    "\n",
    "for param_values in param_combinations:\n",
    "    params = dict(zip(param_names, param_values))\n",
    "    print(f\"\\nTraining with parameters: {params}\")\n",
    "\n",
    "    # Rebuild model and optimizer each time\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    # --- Freeze Layers ---\n",
    "    if params[\"num_frozen_layers\"] > 0:\n",
    "        for idx, layer in enumerate(model.roberta.encoder.layer):\n",
    "            if idx < params[\"num_frozen_layers\"]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "        print(f\"✅ Froze first {params['num_frozen_layers']} layers.\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                  lr=params[\"learning_rate\"], \n",
    "                                  weight_decay=params[\"weight_decay\"])\n",
    "\n",
    "    # Rebuild loaders if batch_size changes\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # --- Training ---\n",
    "    for epoch in range(2):  # Short training for search\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "            inputs = {k: v for k, v in batch.items() if k != \"video_ids\"}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "            inputs = {k: v for k, v in batch.items() if k != \"video_ids\"}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\n Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\" Best Parameters: {best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exist2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "437ffc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae42e63",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c9735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(775, 18)\n",
      "A woman in a TikTok video recounts an interaction with a man who said he doesn't like women who are beautiful and know it, and she responds by deconstructing his statement as a reflection of his own insecurities. The video criticizes sexism by highlighting and challenging a man's prejudiced statement against confident, beautiful women. It reframes his dislike as an insecurity stemming from the belief that he has nothing to offer such women, thereby critiquing the misogynistic view that women's self-awareness of their beauty is a negative trait. The video features a woman recounting and satirically critiquing a man's sexist comments about women's beauty and confidence, with the video's framing clearly opposing and mocking these sexist views.\n"
     ]
    }
   ],
   "source": [
    "test_path = \"/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/train_test_split/test_df.csv\"\n",
    "val_path = \"/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/train_test_split/valid_df.csv\"\n",
    "train_path = \"/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/train_test_split/train_df.csv\"\n",
    "\n",
    "# Read the CSV files\n",
    "test_df = pd.read_csv(test_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "# Combine all dataframes\n",
    "df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Drop the individual columns since we've combined them\n",
    "# Concatenate description_fp, analysis_fp, and analysis_fn into text column\n",
    "df['text'] = df['description_fp'] + ' ' + df['analysis_fp'] + ' ' + df['analysis_fn']\n",
    "test_df['text'] = test_df['description_fp'] + ' ' + test_df['analysis_fp'] + ' ' + test_df['analysis_fn']\n",
    "\n",
    "# description_fp+analysis_fp+description_fn (Mean Test Accuracy: 0.8216 Â± 0.0177)\n",
    "# df['text'] = df['description_fp'] + ' ' + df['analysis_fp'] + ' ' + df['description_fn']\n",
    "# test_df['text'] = test_df['description_fp'] + ' ' + test_df['analysis_fp'] + ' ' + test_df['description_fn']\n",
    "\n",
    "# Drop the individual columns since we've combined them\n",
    "df = df.drop(columns=['description_fp', 'analysis_fp', 'description_fn'])\n",
    "\n",
    "# Convert target values to YES/NO for both dataframes\n",
    "df['target'] = df['target'].map({1: 'YES', 0: 'NO'})\n",
    "test_df['target'] = test_df['target'].map({1: 'YES', 0: 'NO'})\n",
    "\n",
    "# Remove rows where text is NA\n",
    "df = df.dropna(subset=['text'])\n",
    "test_df = test_df.dropna(subset=['text'])\n",
    "\n",
    "# Map target values to 1/0 for both dataframes\n",
    "df['target'] = df['target'].map({'YES': 1, 'NO': 0})\n",
    "test_df['target'] = test_df['target'].map({'YES': 1, 'NO': 0})\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "print(df['text'].iloc[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812687c",
   "metadata": {},
   "source": [
    "# Stratified KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7a345de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, video_ids, exist_ids):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.video_ids = video_ids\n",
    "        self.exist_ids = exist_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['video_ids'] = self.video_ids[idx]\n",
    "        item['exist_ids'] = self.exist_ids[idx]\n",
    "        return item\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "def get_loader(df, batch_size=16, shuffle=False):\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['target'].tolist()\n",
    "    video_ids = df['video'].tolist()\n",
    "    exist_ids = df['id_EXIST'].tolist()\n",
    "    encodings = tokenize(texts)\n",
    "    dataset = TextDataset(encodings, labels, video_ids, exist_ids)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fe5c0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(train_loader, val_loader, device, num_labels=2, freeze_layers_up_to=20, epochs=6):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels)\n",
    "    \n",
    "    # Freeze embeddings and encoder layers up to `freeze_layers_up_to`\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"roberta.embeddings\"):\n",
    "            param.requires_grad = False\n",
    "        elif \"roberta.encoder.layer\" in name:\n",
    "            layer_num = int(name.split(\"layer.\")[1].split(\".\")[0])\n",
    "            if layer_num <= freeze_layers_up_to:\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Train\"):\n",
    "            batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "            inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "            outputs = model(**inputs)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (torch.argmax(logits, dim=1) == batch['labels']).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Val\"):\n",
    "                batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "                inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "                outputs = model(**inputs)\n",
    "                val_loss += outputs.loss.item()\n",
    "                correct += (torch.argmax(outputs.logits, dim=1) == batch['labels']).sum().item()\n",
    "                total += batch['labels'].size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "        print(f\"[Epoch {epoch+1}] Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "460d50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def run_kfold(df, test_df, k=5, device='cuda'):\n",
    "    \n",
    "    # Split the data into train+validation and test sets\n",
    "    df_train_valid = df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    print(f\"Train+Validation set shape: {df_train_valid.shape}\")\n",
    "    print(f\"Test set shape: {test_df.shape}\")\n",
    "    print(\"Columns in df_train_valid:\")\n",
    "    print(df_train_valid.columns.tolist())\n",
    "    \n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(df_train_valid)):\n",
    "        print(f\"\\n===== Fold {fold_idx+1} =====\")\n",
    "        train_df = df_train_valid.iloc[train_idx]\n",
    "        val_df = df_train_valid.iloc[val_idx]\n",
    "\n",
    "        print(f\"Train set shape: {train_df.shape}\")\n",
    "        print(f\"Validation set shape: {val_df.shape}\")\n",
    "\n",
    "        train_loader = get_loader(train_df, shuffle=True)\n",
    "        val_loader = get_loader(val_df)\n",
    "        model = train_and_validate(train_loader, val_loader, device)\n",
    "        # Save the model for this fold\n",
    "        model_save_path = f'models/roberta_large_fold_{fold_idx+1}.pt'\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "        # Optional: Evaluate on the held-out test set\n",
    "        test_loader = get_loader(test_df)\n",
    "        model.eval()\n",
    "        preds, targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "                batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "                inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "                outputs = model(**inputs)\n",
    "                preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                targets.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        test_acc = accuracy_score(targets, preds)\n",
    "        print(f\"Fold {fold_idx+1} Test Accuracy: {test_acc:.4f}\")\n",
    "        fold_results.append(test_acc)\n",
    "\n",
    "    print(\"\\n=== K-Fold Summary ===\")\n",
    "    print(f\"Mean Test Accuracy: {np.mean(fold_results):.4f} Â± {np.std(fold_results):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9bacd6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Validation set shape: (775, 18)\n",
      "Test set shape: (194, 21)\n",
      "Columns in df_train_valid:\n",
      "['id_Tiktok', 'id_EXIST', 'lang', 'text', 'video', 'path_video', 'url', 'annotators', 'number_annotators', 'gender_annotators', 'labels_task3_1', 'labels_task3_2', 'labels_task3_3', 'split', 'target', 'label_fp', 'label_fn', 'analysis_fn']\n",
      "\n",
      "===== Fold 1 =====\n",
      "Train set shape: (620, 18)\n",
      "Validation set shape: (155, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|ââââââââââ| 39/39 [01:01<00:00,  1.57s/it]\n",
      "Epoch 1 - Val: 100%|ââââââââââ| 10/10 [00:08<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.5403 | Val Acc: 0.5677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|ââââââââââ| 39/39 [00:37<00:00,  1.03it/s]\n",
      "Epoch 2 - Val: 100%|ââââââââââ| 10/10 [00:07<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.6500 | Val Acc: 0.8129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|ââââââââââ| 39/39 [00:37<00:00,  1.04it/s]\n",
      "Epoch 3 - Val: 100%|ââââââââââ| 10/10 [00:07<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.8113 | Val Acc: 0.8323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|ââââââââââ| 39/39 [00:37<00:00,  1.03it/s]\n",
      "Epoch 4 - Val: 100%|ââââââââââ| 10/10 [00:07<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.8097 | Val Acc: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|ââââââââââ| 39/39 [00:40<00:00,  1.04s/it]\n",
      "Epoch 5 - Val: 100%|ââââââââââ| 10/10 [00:08<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8629 | Val Acc: 0.8516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|ââââââââââ| 39/39 [00:40<00:00,  1.03s/it]\n",
      "Epoch 6 - Val: 100%|ââââââââââ| 10/10 [00:07<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Acc: 0.9210 | Val Acc: 0.8516\n",
      "Model saved to models/roberta_large_fold_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|ââââââââââ| 13/13 [00:21<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test Accuracy: 0.8454\n",
      "\n",
      "===== Fold 2 =====\n",
      "Train set shape: (620, 18)\n",
      "Validation set shape: (155, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|ââââââââââ| 39/39 [00:47<00:00,  1.22s/it]\n",
      "Epoch 1 - Val: 100%|ââââââââââ| 10/10 [00:07<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.5081 | Val Acc: 0.5742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|ââââââââââ| 39/39 [00:42<00:00,  1.08s/it]\n",
      "Epoch 2 - Val: 100%|ââââââââââ| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.5855 | Val Acc: 0.6065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|ââââââââââ| 39/39 [00:41<00:00,  1.06s/it]\n",
      "Epoch 3 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.7387 | Val Acc: 0.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|ââââââââââ| 39/39 [00:41<00:00,  1.07s/it]\n",
      "Epoch 4 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.7774 | Val Acc: 0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|ââââââââââ| 39/39 [00:41<00:00,  1.06s/it]\n",
      "Epoch 5 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8032 | Val Acc: 0.8129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|ââââââââââ| 39/39 [00:41<00:00,  1.05s/it]\n",
      "Epoch 6 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Acc: 0.8581 | Val Acc: 0.8194\n",
      "Model saved to models/roberta_large_fold_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|ââââââââââ| 13/13 [00:18<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Test Accuracy: 0.8144\n",
      "\n",
      "===== Fold 3 =====\n",
      "Train set shape: (620, 18)\n",
      "Validation set shape: (155, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|ââââââââââ| 39/39 [00:53<00:00,  1.38s/it]\n",
      "Epoch 1 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.5290 | Val Acc: 0.5677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|ââââââââââ| 39/39 [00:43<00:00,  1.13s/it]\n",
      "Epoch 2 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.6726 | Val Acc: 0.7290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|ââââââââââ| 39/39 [00:49<00:00,  1.28s/it]\n",
      "Epoch 3 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.7694 | Val Acc: 0.7161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|ââââââââââ| 39/39 [00:47<00:00,  1.23s/it]\n",
      "Epoch 4 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.8145 | Val Acc: 0.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|ââââââââââ| 39/39 [00:55<00:00,  1.42s/it]\n",
      "Epoch 5 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8387 | Val Acc: 0.7290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|ââââââââââ| 39/39 [00:51<00:00,  1.31s/it]\n",
      "Epoch 6 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Acc: 0.8726 | Val Acc: 0.7290\n",
      "Model saved to models/roberta_large_fold_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|ââââââââââ| 13/13 [00:14<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Test Accuracy: 0.8196\n",
      "\n",
      "===== Fold 4 =====\n",
      "Train set shape: (620, 18)\n",
      "Validation set shape: (155, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|ââââââââââ| 39/39 [00:51<00:00,  1.33s/it]\n",
      "Epoch 1 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.5339 | Val Acc: 0.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|ââââââââââ| 39/39 [00:42<00:00,  1.08s/it]\n",
      "Epoch 2 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.5774 | Val Acc: 0.6903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|ââââââââââ| 39/39 [00:42<00:00,  1.09s/it]\n",
      "Epoch 3 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.6565 | Val Acc: 0.6968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|ââââââââââ| 39/39 [00:42<00:00,  1.08s/it]\n",
      "Epoch 4 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.7887 | Val Acc: 0.7032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|ââââââââââ| 39/39 [00:41<00:00,  1.06s/it]\n",
      "Epoch 5 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8419 | Val Acc: 0.7484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|ââââââââââ| 39/39 [00:41<00:00,  1.06s/it]\n",
      "Epoch 6 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Acc: 0.8871 | Val Acc: 0.7677\n",
      "Model saved to models/roberta_large_fold_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|ââââââââââ| 13/13 [00:21<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Test Accuracy: 0.8351\n",
      "\n",
      "===== Fold 5 =====\n",
      "Train set shape: (620, 18)\n",
      "Validation set shape: (155, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Train: 100%|ââââââââââ| 39/39 [04:04<00:00,  6.27s/it]\n",
      "Epoch 1 - Val: 100%|ââââââââââ| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Acc: 0.5597 | Val Acc: 0.5161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|ââââââââââ| 39/39 [03:11<00:00,  4.90s/it]\n",
      "Epoch 2 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Acc: 0.7016 | Val Acc: 0.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|ââââââââââ| 39/39 [02:41<00:00,  4.13s/it]\n",
      "Epoch 3 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Acc: 0.7871 | Val Acc: 0.8323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|ââââââââââ| 39/39 [00:49<00:00,  1.28s/it]\n",
      "Epoch 4 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Acc: 0.8274 | Val Acc: 0.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|ââââââââââ| 39/39 [17:37<00:00, 27.11s/it]   \n",
      "Epoch 5 - Val: 100%|ââââââââââ| 10/10 [00:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Acc: 0.8500 | Val Acc: 0.8323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|ââââââââââ| 39/39 [03:12<00:00,  4.92s/it]\n",
      "Epoch 6 - Val: 100%|ââââââââââ| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Acc: 0.9081 | Val Acc: 0.7871\n",
      "Model saved to models/roberta_large_fold_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|ââââââââââ| 13/13 [00:16<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Test Accuracy: 0.7938\n",
      "\n",
      "=== K-Fold Summary ===\n",
      "Mean Test Accuracy: 0.8216 Â± 0.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_kfold(df, test_df, k=5, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "abe6c647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases where label_fp equals label_fn: 103\n",
      "Accuracy of these labels compared to target: 85.44%\n",
      "Total rows in test set: 194\n",
      "Rows where fp equals fn: 103\n",
      "Percentage: 53.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b3/39sggkjn14n1xr2hwd01qgpw0000gp/T/ipykernel_77012/4252406209.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matching_labels['target_label'] = matching_labels['target'].map({1: 'YES', 0: 'NO'})\n"
     ]
    }
   ],
   "source": [
    "# Check where label_fp equals label_fn\n",
    "matching_labels = test_df[test_df['label_fp'] == test_df['label_fn']]\n",
    "\n",
    "# Calculate accuracy between matching labels and target\n",
    "# Convert target to YES/NO labels\n",
    "matching_labels['target_label'] = matching_labels['target'].map({1: 'YES', 0: 'NO'})\n",
    "correct_predictions = (matching_labels['label_fp'] == matching_labels['target_label']).sum()\n",
    "total_predictions = len(matching_labels)\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "print(f\"Number of cases where label_fp equals label_fn: {total_predictions}\")\n",
    "print(f\"Accuracy of these labels compared to target: {accuracy:.2%}\")\n",
    "\n",
    "# Calculate percentage of rows where fp equals fn\n",
    "total_rows = len(test_df)\n",
    "matching_rows = len(matching_labels)\n",
    "percentage = (matching_rows / total_rows) * 100\n",
    "\n",
    "print(f\"Total rows in test set: {total_rows}\")\n",
    "print(f\"Rows where fp equals fn: {matching_rows}\")\n",
    "print(f\"Percentage: {percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0943a998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting non-matching labels: 100%|ââââââââââ| 6/6 [00:29<00:00,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-matching labels accuracy: 0.7582\n",
      "\n",
      "Model agreement with label_fp: 65.93%\n",
      "Model agreement with label_fn: 34.07%\n",
      "\n",
      "Number of cases where model disagrees with both fp and fn: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/b3/39sggkjn14n1xr2hwd01qgpw0000gp/T/ipykernel_77012/3409687759.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_matching_labels['model_prediction'] = non_matching_preds\n",
      "/var/folders/b3/39sggkjn14n1xr2hwd01qgpw0000gp/T/ipykernel_77012/3409687759.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_matching_labels['model_prediction_label'] = non_matching_labels['model_prediction'].map({1: 'YES', 0: 'NO'})\n"
     ]
    }
   ],
   "source": [
    "# Get rows where label_fp does not equal label_fn\n",
    "non_matching_labels = test_df[test_df['label_fp'] != test_df['label_fn']]\n",
    "\n",
    "# Create a new DataFrame with just the text and target columns for non-matching labels\n",
    "non_matching_df = non_matching_labels[['text', 'target','video','id_EXIST']].copy()\n",
    "\n",
    "# Create a new dataloader for non-matching labels\n",
    "non_matching_loader = get_loader(non_matching_df)\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=2)\n",
    "# model.load_state_dict(torch.load('/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/gemini/models/roberta_large_fold_4.pt'))\n",
    "# model = model.to(device)\n",
    "\n",
    "state_dict = torch.load('/Volumes/T7/OMSCS/CLEF2025/EXIST2025/exist-2025/notebooks/gemini/models/roberta_large_fold_5.pt', map_location='cpu')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=2)\n",
    "\n",
    "# Then load weights\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Now try moving to MPS (may still fail, but less likely)\n",
    "model = model.to(device)\n",
    "\n",
    "# Get predictions for non-matching labels\n",
    "model.eval()\n",
    "non_matching_preds, non_matching_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(non_matching_loader, desc=\"Predicting non-matching labels\"):\n",
    "        batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        inputs = {k: v for k, v in batch.items() if k not in [\"video_ids\", \"exist_ids\"]}\n",
    "        outputs = model(**inputs)\n",
    "        non_matching_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        non_matching_targets.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "# Calculate accuracy for non-matching labels\n",
    "non_matching_acc = accuracy_score(non_matching_targets, non_matching_preds)\n",
    "print(f\"\\nNon-matching labels accuracy: {non_matching_acc:.4f}\")\n",
    "\n",
    "# Add predictions back to the original DataFrame\n",
    "non_matching_labels['model_prediction'] = non_matching_preds\n",
    "non_matching_labels['model_prediction_label'] = non_matching_labels['model_prediction'].map({1: 'YES', 0: 'NO'})\n",
    "\n",
    "# Compare model predictions with fp and fn labels\n",
    "fp_agreement = (non_matching_labels['model_prediction_label'] == non_matching_labels['label_fp']).mean()\n",
    "fn_agreement = (non_matching_labels['model_prediction_label'] == non_matching_labels['label_fn']).mean()\n",
    "\n",
    "print(f\"\\nModel agreement with label_fp: {fp_agreement:.2%}\")\n",
    "print(f\"Model agreement with label_fn: {fn_agreement:.2%}\")\n",
    "\n",
    "# Display some examples where model disagrees with both fp and fn\n",
    "disagreements = non_matching_labels[\n",
    "    (non_matching_labels['model_prediction_label'] != non_matching_labels['label_fp']) & \n",
    "    (non_matching_labels['model_prediction_label'] != non_matching_labels['label_fn'])\n",
    "]\n",
    "\n",
    "print(f\"\\nNumber of cases where model disagrees with both fp and fn: {len(disagreements)}\")\n",
    "if len(disagreements) > 0:\n",
    "    print(\"\\nExample disagreements:\")\n",
    "    for _, row in disagreements.head(3).iterrows():\n",
    "        print(f\"\\nText: {row['text']}\")\n",
    "        print(f\"Target: {row['target_label']}\")\n",
    "        print(f\"Model prediction: {row['model_prediction_label']}\")\n",
    "        print(f\"label_fp: {row['label_fp']}\")\n",
    "        print(f\"label_fn: {row['label_fn']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exist2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
